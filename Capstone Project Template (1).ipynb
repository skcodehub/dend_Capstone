{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, udf, col, min, max, when\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql import SQLContext\n",
    "import psycopg2\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dwh.cfg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID']=config['keypair']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['keypair']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20555.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AZ</td>\n",
       "      <td>9.247104e+10</td>\n",
       "      <td>00602</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20562.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AZ</td>\n",
       "      <td>9.247849e+10</td>\n",
       "      <td>00608</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>27.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20549.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>04062016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>9.247876e+10</td>\n",
       "      <td>00422</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>28.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20549.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>04062016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>9.247890e+10</td>\n",
       "      <td>00422</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>20554.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>9.248056e+10</td>\n",
       "      <td>00065</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "5    18.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MI   \n",
       "9    22.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      NY   \n",
       "12   27.0  2016.0     4.0   101.0   101.0     BOS  20545.0      1.0      MA   \n",
       "13   28.0  2016.0     4.0   101.0   101.0     ATL  20545.0      1.0      MA   \n",
       "24   40.0  2016.0     4.0   101.0   101.0     CHI  20545.0      1.0      IL   \n",
       "\n",
       "    depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "5   20555.0   ...         NaN        M   1959.0  09302016    NaN    NaN   \n",
       "9   20562.0   ...         NaN        M   1968.0  09302016    NaN    NaN   \n",
       "12  20549.0   ...         NaN        M   1958.0  04062016      M    NaN   \n",
       "13  20549.0   ...         NaN        M   1960.0  04062016      F    NaN   \n",
       "24  20554.0   ...         NaN        M   1981.0  09302016      M    NaN   \n",
       "\n",
       "   airline        admnum  fltno visatype  \n",
       "5       AZ  9.247104e+10  00602       B1  \n",
       "9       AZ  9.247849e+10  00608       B1  \n",
       "12      LH  9.247876e+10  00422       B1  \n",
       "13      LH  9.247890e+10  00422       B1  \n",
       "24      OS  9.248056e+10  00065       B1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter = df[(df.visatype)=='B1']\n",
    "df_filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.2\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data_parquet\", \"overwrite\")\n",
    "#df_spark.write.partitionBy(\"i94yr\",\"i94mon\").parquet(\"sas_data_parquet_partition\", \"overwrite\")\n",
    "df_spark=spark.read.parquet(\"sas_data_parquet_clean/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|visatype|\n",
      "+--------+\n",
      "|      F2|\n",
      "|     GMB|\n",
      "|      B2|\n",
      "|      F1|\n",
      "|     CPL|\n",
      "|      I1|\n",
      "|      WB|\n",
      "|      M1|\n",
      "|      B1|\n",
      "|      WT|\n",
      "|      M2|\n",
      "|      CP|\n",
      "|     GMT|\n",
      "|      E1|\n",
      "|       I|\n",
      "|      E2|\n",
      "|     SBP|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_filter = df_spark[(df_spark.visatype)=='B1']\n",
    "#df_filter.head()\n",
    "#Check distinct visa types to check if there are any nulls as this would be used to draw metrics on immigrant type\n",
    "df_spark.select('visatype').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 1 1\n"
     ]
    }
   ],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "epoch = datetime.datetime(1960, 1, 1)\n",
    "get_datetime = udf(lambda x: epoch + datetime.timedelta(days=int(x)))\n",
    "df=df_spark.select(get_datetime(df_spark.arrdate).alias(\"Arrival_Date\"), df_spark.arrdate)\n",
    "\n",
    "df.head()\n",
    "#From the above it seems like Mar 30, 2016 has code 20574.So removing 90 days from this would be 20484 which should be \n",
    "# jan 1, 2016\n",
    "#test_date_id = 20484\n",
    "test_date_id = 20454\n",
    "print((epoch + datetime.timedelta(days=test_date_id)).year, (epoch + datetime.timedelta(days=test_date_id)).month,(epoch + datetime.timedelta(days=test_date_id)).day)\n",
    "# Using 20454 as start date id, dim_date can be created from 2016-2020 with date_id+1 and counting down to 1900-1-1 as best practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-01 00:00:00\n",
      "Column<b'to_date(`01/01/2016`)'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "field calendar_date: This field is not nullable, but got None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-6cc5d9c9a672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mnewRow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstart_date_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweekday\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtemp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewRow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewRow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdate_id\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdate_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;31m# make sure data could consumed multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mverify_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m             \u001b[0mverify_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifier\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mverifiers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                     \u001b[0mverifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m                 raise TypeError(new_msg(\"StructType can not accept object %r in type %s\"\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mverify_nullability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m             \u001b[0mverify_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36mverify_nullability\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1262\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This field is not nullable, but got None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: field calendar_date: This field is not nullable, but got None"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, TimestampType\n",
    "#from pyspark.sql.functions import to_date\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "epoch = datetime.datetime(1960, 1, 1)\n",
    "start_date = datetime.datetime(2016,1,1)\n",
    "#end_date = datetime.datetime(2020,12,31)\n",
    "end_date = datetime.datetime(2016,1,10)\n",
    "date_id = 20454\n",
    "interval = datetime.timedelta(days =1)\n",
    "\n",
    "#epoch = to_date(\"1960-01-01\")\n",
    "#start_date =to_date(\"2016-01-01\")\n",
    "#end_date = to_date(\"2016-12-31\")\n",
    "#num_days =1\n",
    "\n",
    "schema = StructType([StructField(\"calendar_date\", DateType(), False),\n",
    "                     StructField(\"date_id\", IntegerType(), False),\n",
    "                     StructField(\"day\", IntegerType(), False),\n",
    "                     StructField(\"month\", IntegerType(), False),\n",
    "                     StructField(\"year\", IntegerType(), False),\n",
    "                     StructField(\"weekday\", IntegerType(), False)]\n",
    "                     )\n",
    "#columns = ['calendar_date', 'date_id', 'day', 'month', 'year', 'weekday']\n",
    "df = spark.createDataFrame([], schema)\n",
    "#start_date = start_date.strftime(\"%m/%d/%Y %H:%M\")\n",
    "print(start_date)\n",
    "while start_date <= end_date:\n",
    "    #cur.execute(date_dim_insert, [start_date, date_id, start_date.day, start_date.month, start_date.year, start_date.weekday()])\n",
    "    #conn.commit()\n",
    "    start_date_ts = f.to_date(start_date.strftime(\"%m/%d/%Y\"))\n",
    "    print(start_date_ts)\n",
    "    newRow = [start_date_ts, date_id, start_date.day, start_date.month, start_date.year, start_date.weekday()]\n",
    "    temp_df = spark.createDataFrame(newRow, schema)\n",
    "    df = df.union(newRow)\n",
    "    date_id= date_id+1\n",
    "    start_date = start_date+interval\n",
    "#    start_date=date_add(start_date, num_days)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Use this section to load all the necessary files to S3 for further processing\n",
    "s3 = boto3.resource('s3')\n",
    "BUCKET = \"capstoneimmi\"\n",
    "\n",
    "#Load demographic data to S3\n",
    "s3.Bucket(BUCKET).upload_file(\"us-cities-demographics.csv\", \"demo/us-cities-demographics.csv\")\n",
    "\n",
    "#Load I94_Port data to S3\n",
    "s3.Bucket(BUCKET).upload_file(\"I94_Port.csv\", \"port/I94_Port.csv\")\n",
    "\n",
    "#Load I94_Mode data to S3\n",
    "s3.Bucket(BUCKET).upload_file(\"I94_Mode.csv\", \"mode/I94_Mode.csv\")\n",
    "\n",
    "#Load I94_Visa data to S3\n",
    "s3.Bucket(BUCKET).upload_file(\"I94_Visa.csv\", \"visa/I94_Visa.csv\")\n",
    "\n",
    "#Load I94ADDR_State data to S3\n",
    "s3.Bucket(BUCKET).upload_file(\"I94ADDR_State.csv\", \"addrstate/I94ADDR_State.csv\")\n",
    "\n",
    "#Load I94City_Res data to S3\n",
    "s3.Bucket(BUCKET).upload_file(\"I94City_Res.csv\", \"rescitycntry/I94City_Res.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Load immi data to S3\n",
    "#df_spark.write.parquet(\"s3a://capstoneimmi/sas-data-parquet/\", mode=\"overwrite\")\n",
    "\n",
    "#from glob import glob\n",
    "#filenames = glob('sas_data_parquet/part*.parquet')\n",
    "#for f in filenames:\n",
    "#    df=pd.append.parquet(\"sas_data_parquet\")\n",
    "\n",
    "#df_spark.write.parquet(\"sas_data_parquet\", \"overwrite\")\n",
    "df_spark.write.parquet(\"s3a://capstoneimmi/sas-data-parquet/\", mode=\"overwrite\")\n",
    "#df_spark=spark.read.parquet(\"sas_data_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#May not use this\n",
    "fname = 'us-cities-demographics.csv'\n",
    "df = pd.read_csv(fname, delimiter=';')\n",
    "df.head()\n",
    "\n",
    "# Some of the data elements in this file is a snapshot in time as they are subject to change over time. For example, population, number of veterans, foreign born,etc. \n",
    "# So we will use distinct state_code, city, state to create dim_city_state. State_code, city with other elements could be used to create a snapshot fact if date/time element was in the data. \n",
    "# However, since the data doesn't have a date element, we can create a demographic_ref_dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  value                      i94prtl\n",
      "0                   ALC                     ALCAN,AK\n",
      "1                   ANC                 ANCHORAGE,AK\n",
      "2                   BAR      BAKERAAF-BAKERISLAND,AK\n",
      "3                   DAC              DALTONSCACHE,AK\n",
      "4                   PIZ        DEWSTATIONPTLAYDEW,AK\n",
      "5                   DTH               DUTCHHARBOR,AK\n",
      "6                   EGL                     EAGLE,AK\n",
      "7                   FRB                 FAIRBANKS,AK\n",
      "8                   HOM                     HOMER,AK\n",
      "9                   HYD                     HYDER,AK\n",
      "10                  JUN                    JUNEAU,AK\n",
      "11                  5KE                 KETCHIKAN,AK\n",
      "12                  KET                 KETCHIKAN,AK\n",
      "13                  MOS    MOSESPOINTINTERMEDIATE,AK\n",
      "14                  NIK                   NIKISKI,AK\n",
      "15                  NOM                       NOM,AK\n",
      "16                  PKC                POKERCREEK,AK\n",
      "17                  ORI              PORTLIONSSPB,AK\n",
      "18                  SKA                   SKAGWAY,AK\n",
      "19                  SNP             ST.PAULISLAND,AK\n",
      "20                  TKI                    TOKEEN,AK\n",
      "21                  WRA                  WRANGELL,AK\n",
      "22                  HSV  MADISONCOUNTY-HUNTSVILLE,AL\n",
      "23                  MOB                    MOBILE,AL\n",
      "24                  LIA           LITTLEROCK,AR(BPS)\n",
      "25                  ROG                ROGERSARPT,AR\n",
      "26                  DOU                   DOUGLAS,AZ\n",
      "27                  LUK                 LUKEVILLE,AZ\n",
      "28                  MAP                   MARIPOSAAZ\n",
      "29                  NAC                      NACO,AZ\n",
      "..                  ...                          ...\n",
      "630                 SCH              NoPORTCode(SCH)\n",
      "631                 ASI              NoPORTCode(ASI)\n",
      "632                 BKF              NoPORTCode(BKF)\n",
      "633                 DAY              NoPORTCode(DAY)\n",
      "634                 Y62              NoPORTCode(Y62)\n",
      "635  AG\\tNoPORTCode(AG)                          NaN\n",
      "636                 BCM              NoPORTCode(BCM)\n",
      "637                 DEC              NoPORTCode(DEC)\n",
      "638                 PLB              NoPORTCode(PLB)\n",
      "639                 CXO              NoPORTCode(CXO)\n",
      "640                 JBQ              NoPORTCode(JBQ)\n",
      "641                 JIG              NoPORTCode(JIG)\n",
      "642                 OGS              NoPORTCode(OGS)\n",
      "643                 TIW              NoPORTCode(TIW)\n",
      "644                 OTS              NoPORTCode(OTS)\n",
      "645                 AMT              NoPORTCode(AMT)\n",
      "646                 EGE              NoPORTCode(EGE)\n",
      "647                 GPI              NoPORTCode(GPI)\n",
      "648                 NGL              NoPORTCode(NGL)\n",
      "649                 OLM              NoPORTCode(OLM)\n",
      "650                 .GA              NoPORTCode(.GA)\n",
      "651                 CLX              NoPORTCode(CLX)\n",
      "652                  CP               NoPORTCode(CP)\n",
      "653                 FSC              NoPORTCode(FSC)\n",
      "654                  NK               NoPORTCode(NK)\n",
      "655                 ADU              NoPORTCode(ADU)\n",
      "656                 AKT              NoPORTCode(AKT)\n",
      "657                 LIT              NoPORTCode(LIT)\n",
      "658                 A2A              NoPORTCode(A2A)\n",
      "659                 OSN              NoPORTCode(OSN)\n",
      "\n",
      "[660 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "fname = 'I94_Port.csv'\n",
    "df = pd.read_csv(fname, delimiter=';')\n",
    "df.head()\n",
    "#remove single quotes from value and i94prtl so that it can be loaded into the db\n",
    "df['value'] = df['value'].str.replace(r\"[\\',]\", '')\n",
    "#print(df)\n",
    "df['i94prtl'] = df['i94prtl'].str.replace(r\"[\\']\",'')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "| dtaddto|\n",
      "+--------+\n",
      "|     183|\n",
      "|10 02003|\n",
      "|     D/S|\n",
      "|06 02002|\n",
      "|/   183D|\n",
      "|12319999|\n",
      "+--------+\n",
      "\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "| CICID| I94YR|I94MON|I94CIT|I94RES|I94PORT|ARRDATE|I94MODE|I94ADDR|DEPDATE|I94BIR|I94VISA|COUNT|DTADFILE|VISAPOST|OCCUP|ENTDEPA|ENTDEPD|ENTDEPU|MATFLAG|BIRYEAR| DTADDTO|GENDER|INSNUM|AIRLINE|         ADMNUM|FLTNO|VISATYPE|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "| 100.0|2016.0|   4.0| 103.0| 103.0|    DAL|20545.0|    1.0|     TX|20560.0|  31.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1985.0|06292016|  null|  null|     BA|5.5447764233E10|00193|      WT|\n",
      "| 141.0|2016.0|   4.0| 103.0| 103.0|    NEW|20545.0|    1.0|     NY|20549.0|  20.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1996.0|06292016|     F|  null|     OS|5.5422561433E10|00089|      WT|\n",
      "| 461.0|2016.0|   4.0| 103.0| 103.0|    SFR|20545.0|    1.0|     FL|20546.0|  45.0|    2.0|  1.0|20160401|    null| null|      O|      R|   null|      M| 1971.0|06292016|  null|  null|     OS|5.5426320733E10|00097|      WT|\n",
      "| 869.0|2016.0|   4.0| 104.0| 104.0|    NEW|20545.0|    1.0|     FL|20549.0|  19.0|    2.0|  1.0|20160401|    null| null|      G|      I|   null|      M| 1997.0|06292016|     F|  null|     LH|5.5438094333E10|00402|      WT|\n",
      "|1254.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20548.0|  21.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1995.0|06292016|     M|  null|     KL|5.5448241933E10|00641|      WT|\n",
      "|1461.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20551.0|  13.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 2003.0|06292016|     M|  null|     BA|5.5414926533E10|00175|      WT|\n",
      "|1505.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20552.0|  13.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 2003.0|06292016|     F|  null|     SN|5.5420121233E10|01401|      WT|\n",
      "|1516.0|2016.0|   4.0| 104.0| 104.0|    NYC|20545.0|    1.0|     NY|20554.0|  52.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1964.0|06292016|     M|  null|     LH|5.5436801933E10|00400|      WT|\n",
      "|1911.0|2016.0|   4.0| 104.0| 104.0|    SFR|20545.0|    1.0|     CA|20668.0|  23.0|    1.0|  1.0|20160401|     BRS| null|      T|      O|   null|      M| 1993.0|09302016|     M|  null|     UA|   6.65401085E8| 1159|      B1|\n",
      "|1939.0|2016.0|   4.0| 104.0| 111.0|    LOS|20545.0|    1.0|     CA|20553.0|  26.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1990.0|06292016|     F|  null|     AF|5.5442616233E10|00066|      WT|\n",
      "|2253.0|2016.0|   4.0| 107.0| 107.0|    FTL|20545.0|    1.0|     FL|20546.0|  31.0|    2.0|  1.0|20160401|    null| null|      O|      I|   null|      M| 1985.0|09302016|  null|  null|     DY| 9.251211943E10|07031|      B2|\n",
      "|2309.0|2016.0|   4.0| 107.0| 107.0|    WAS|20545.0|    1.0|   null|20565.0|  47.0|    1.0|  1.0|20160401|     BLF| null|      G|      I|   null|      M| 1969.0|09302016|     M|  null|     UA| 9.248099593E10|00107|      B1|\n",
      "|2328.0|2016.0|   4.0| 107.0| 107.0|    BOS|20545.0|    1.0|     MA|20551.0|  31.0|    1.0|  1.0|20160401|     FRN| null|      G|      O|   null|      M| 1985.0|09302016|     M|  null|     LH| 9.250370173E10|00424|      B1|\n",
      "|2517.0|2016.0|   4.0| 107.0| 107.0|    BOS|20545.0|    1.0|     MA|20549.0|  55.0|    1.0|  1.0|20160401|     WRW| null|      G|      O|   null|      M| 1961.0|04302016|     M|  null|     LH| 9.247900023E10|00422|      B1|\n",
      "|2524.0|2016.0|   4.0| 107.0| 107.0|    CLT|20545.0|    1.0|     VA|20567.0|  29.0|    1.0|  1.0|20160401|     WRW| null|      G|      O|   null|      M| 1987.0|09302016|     M|  null|     LH| 9.249103603E10|00428|      B1|\n",
      "|3009.0|2016.0|   4.0| 108.0| 108.0|    MIA|20545.0|    1.0|     NY|20554.0|  32.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1984.0|06292016|     M|  null|     SK|5.5415295033E10|00909|      WT|\n",
      "|3134.0|2016.0|   4.0| 108.0| 108.0|    NYC|20545.0|    1.0|     NY|20550.0|  38.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1978.0|06292016|     M|  null|     DY|5.5462903033E10|07011|      WT|\n",
      "|3497.0|2016.0|   4.0| 108.0| 108.0|    SFR|20545.0|    1.0|     CA|20554.0|  83.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1933.0|06292016|     M|  null|     UA|5.5454086533E10|00059|      WT|\n",
      "|3506.0|2016.0|   4.0| 108.0| 108.0|    SFR|20545.0|    1.0|     CA|20555.0|  54.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1962.0|06292016|     F|  null|     SK|5.5447256433E10|00935|      WT|\n",
      "|3573.0|2016.0|   4.0| 108.0| 108.0|    SAJ|20545.0|    1.0|     VQ|20552.0|  47.0|    2.0|  1.0|20160401|    null| null|      G|      O|   null|      M| 1969.0|06292016|     M|  null|     DY|5.5420738133E10|07125|      WT|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "df_spark=spark.read.parquet(\"sas_data_parquet\")\n",
    "df_spark.createOrReplaceTempView(\"immi_fact\")\n",
    "\n",
    "#df_count = spark.sql(\"\"\"select count(*) from immi_fact\"\"\")\n",
    "#df_count.show()\n",
    "#i94 year is used to analyze number of arrivals per year so checking if this column has any null values\n",
    "#df_yr = spark.sql(\"\"\" select count(*) from immi_fact where i94yr is null\"\"\")\n",
    "#df_yr.show()\n",
    "\n",
    "#i94port is used to count arrivals per port per year so checking if there are any nulls here\n",
    "#port_df = spark.sql(\"\"\"select count(*) as null_port from immi_fact where i94port is null\"\"\")\n",
    "#port_df.show()\n",
    "\n",
    "#check if cicid is unique\n",
    "#df_cicid = spark.sql(\"\"\"select count(*) as dup_cicid_count from immi_fact group by cicid having count(*) > 2\"\"\")\n",
    "#df_cicid.show()\n",
    "\n",
    "#2982605 are null which is majority rows so not using this column in the fact \n",
    "#insnum_df = spark.sql(\"\"\"select count(*) as null_insnum from immi_fact where insnum is null\"\"\")\n",
    "#insnum_df.show()\n",
    "\n",
    "#data model will be using i94addr to join to dim to find the number of arrivals so checking if this is null\n",
    "#filter rows from immi_fact where i94addr is null\n",
    "#i94addr_chk = spark.sql(\"\"\"select count(*) as i94addr_chk from immi_fact where i94addr is null\"\"\")\n",
    "#i94addr_chk.show()\n",
    "\n",
    "#to analyze the type of immigrants arriving to a city, we can use i94visa column. check for distinct values in this row\n",
    "#i94visa_chk = spark.sql(\"\"\" select distinct i94visa from immi_fact\"\"\")\n",
    "#i94visa_chk.show()\n",
    "\n",
    "#to analyze the type of immigrants arriving to a city, we can use i94visa column. check for nulls in this row\n",
    "#i94visa_null = spark.sql(\"\"\" select count(*) as null_visa_chk from immi_fact where  i94visa is null\"\"\")\n",
    "#i94visa_null.show()\n",
    "\n",
    "#check valid dates\n",
    "i94_invalid_date = spark.sql(\"\"\" select distinct dtaddto from immi_fact\n",
    "                                    where length(TRIM(dtaddto)) < 8 or\n",
    "                                     substring(DTADDTO, 1,2) NOT IN ('01', '02', '03',\n",
    "                                      '04', '05','06', '07',\n",
    "                                      '08', '09', '10', '11', '12')\n",
    "                                      or \n",
    "                                      substring(DTADDTO, 3,2) NOT IN ('01', '02', '03',\n",
    "                                      '04', '05','06', '07',\n",
    "                                      '08', '09', '10', '11', '12',\n",
    "                                      '11', '12', '13',\n",
    "                                      '14', '15','16', '17',\n",
    "                                      '18', '19', '20', '21', '22',\n",
    "                                      '23','24', '25','26', '27',\n",
    "                                      '28', '29', '30', '31')\n",
    "                                      or                                      \n",
    "                                     substring(DTADDTO, 5,2) NOT IN ('20')\n",
    "                                    \"\"\")\n",
    "i94_invalid_date.show(15)\n",
    "\n",
    "i94_valid_date = spark.sql(\"\"\"SELECT DISTINCT CICID, I94YR, I94MON, I94CIT,I94RES, I94PORT,ARRDATE,I94MODE, \n",
    "                            I94ADDR, DEPDATE, I94BIR, I94VISA, COUNT, \n",
    "                            DTADFILE,\n",
    "                            VISAPOST, OCCUP, ENTDEPA, ENTDEPD, ENTDEPU, MATFLAG, BIRYEAR, \n",
    "                            CASE WHEN TRIM(dtaddto) IN ('183','10 02003','D/S','06 02002','/   183D','12319999') \n",
    "                            THEN NULL \n",
    "                            ELSE dtaddto END as DTADDTO, GENDER, INSNUM, AIRLINE, ADMNUM, FLTNO, VISATYPE\n",
    "                            FROM immi_fact \"\"\")\n",
    "#i94_valid_date = i94_invalid_date.withColumn(\"dtaddto\", function(when((function(col(function(TRIM(\"dtaddto\")))) =='D/S', 'null'))).otherwise(function(col(\"dtaddto\"))))\n",
    "\n",
    "i94_valid_date.show()\n",
    "\n",
    "i94_valid_date.write.parquet(\"sas_data_parquet_clean\", \"overwrite\")\n",
    "#attempt to write clean data to s3\n",
    "#i94_valid_date.write.parquet(\"s3a://capstoneimmi/sas-data-parquet/\", \"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "It needs the following dimensions to support the Fact\n",
    "1. Date_Dim\n",
    "2. City_State_Demographic_Dim\n",
    "3. Visa_Type_Dim\n",
    "4. Port_Dim\n",
    "5. City_Res_Dim\n",
    "6. Addr_State_Dim\n",
    "7. Immigrant_Fact\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "1. Using the SAS integer dates provided, see date_dimension\n",
    "2. After going over the I94_SAS_Labels_Description, I created csv files for Visa_Type, Port, City_Res based on the value and code provided. I will use these CSV files to create the corresponding dimensions tables and load the data.\n",
    "3. Once the dimensions are created, create the fact table based on the metrics to be run and populate the table with data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=studentdb user=student password=student\")\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# create immigration database with UTF8 encoding\n",
    "cur.execute(\"DROP DATABASE IF EXISTS dw_immi\")\n",
    "cur.execute(\"CREATE DATABASE dw_immi WITH ENCODING 'utf8' TEMPLATE template0\")\n",
    "\n",
    "# close connection to default database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# connect to immigration database\n",
    "conn = psycopg2.connect(\"host=127.0.0.1 dbname=dw_immi user=student password=student\")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dimensions and load data into tables\n",
    "#Date_Dim is one of the dimensions that will be used across the database for joining and analyzing data\n",
    "#so seeding this dimension from beginning of time to a high end date helps with analysis\n",
    "\n",
    "#As analyzed above, for arrival and departure dates, the source data uses sas integer date. So using the integer as\n",
    "#date_id, I'm seeding data into the dimension table\n",
    "\n",
    "Date_Dim_drop = \"DROP TABLE IF EXISTS Date_Dim\"\n",
    "\n",
    "Date_Dim_create = (\"\"\"CREATE TABLE IF NOT EXISTS Date_Dim(calendar_date TIMESTAMP(6), date_id INTEGER, \n",
    "     day INTEGER, month INTEGER, year INTEGER, weekday INTEGER, PRIMARY KEY(calendar_date))\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(Date_Dim_drop)\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(Date_Dim_create)\n",
    "conn.commit()\n",
    "\n",
    "Date_Dim_insert = (\"\"\"INSERT INTO Date_Dim(calendar_date, date_id, day, month, year, weekday) VALUES( %s, %s, %s, %s, %s, %s) ON CONFLICT(calendar_date) DO NOTHING\n",
    "\"\"\")\n",
    "epoch = datetime.datetime(1960, 1, 1)\n",
    "start_date = datetime.datetime(2016,1,1)\n",
    "end_date = datetime.datetime(2020,12,31)\n",
    "date_id = 20454\n",
    "interval = datetime.timedelta(days =1)\n",
    "\n",
    "while start_date <= end_date:\n",
    "    cur.execute(Date_Dim_insert, [start_date, date_id, start_date.day, start_date.month, start_date.year, start_date.weekday()])\n",
    "    conn.commit()\n",
    "    date_id= date_id+1\n",
    "    start_date = start_date+interval\n",
    "\n",
    "start_date = datetime.datetime(2015,12,31)\n",
    "end_date = datetime.datetime(1900,1, 1)\n",
    "date_id = 20453\n",
    "interval = datetime.timedelta(days = 1)\n",
    "\n",
    "while start_date >= end_date:\n",
    "    cur.execute(Date_Dim_insert, [start_date, date_id, start_date.day, start_date.month, start_date.year, start_date.weekday()])\n",
    "    conn.commit()\n",
    "    date_id= date_id-1\n",
    "    start_date = start_date-interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(datetime.datetime(1954, 7, 10, 0, 0), -2001, 10, 7, 1954, 5)\n",
      "(datetime.datetime(1954, 7, 9, 0, 0), -2002, 9, 7, 1954, 4)\n",
      "(datetime.datetime(1954, 7, 8, 0, 0), -2003, 8, 7, 1954, 3)\n",
      "(datetime.datetime(1954, 7, 7, 0, 0), -2004, 7, 7, 1954, 2)\n",
      "(datetime.datetime(1954, 7, 6, 0, 0), -2005, 6, 7, 1954, 1)\n",
      "(datetime.datetime(1954, 7, 5, 0, 0), -2006, 5, 7, 1954, 0)\n",
      "(datetime.datetime(1954, 7, 4, 0, 0), -2007, 4, 7, 1954, 6)\n",
      "(datetime.datetime(1954, 7, 3, 0, 0), -2008, 3, 7, 1954, 5)\n",
      "(datetime.datetime(1954, 7, 2, 0, 0), -2009, 2, 7, 1954, 4)\n",
      "(datetime.datetime(1954, 7, 1, 0, 0), -2010, 1, 7, 1954, 3)\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"select * from Date_Dim where date_id < -2000 limit 10\")\n",
    "records = cur.fetchall()\n",
    "for record in records:\n",
    "    print(record)\n",
    "    #print(\"start_date \", record[0])\n",
    "    #print(\"date_id \", record[1])\n",
    "    #print(\"day \", record[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Use distinct state_code, city, state, total population, foreign born to create dim_demograph_city_state\n",
    "fname = 'us-cities-demographics.csv'\n",
    "df = pd.read_csv(fname, delimiter=';')\n",
    "df.head()\n",
    "\n",
    "demographic_dim_drop = \"DROP TABLE IF EXISTS CITY_STATE_DEMOGRAPHIC_DIM\"\n",
    "\n",
    "demographic_dim_create = (\"\"\"CREATE TABLE IF NOT EXISTS CITY_STATE_DEMOGRAPHIC_DIM(\n",
    "    CITY VARCHAR,\n",
    "    STATE_CODE VARCHAR,\n",
    "    STATE VARCHAR,\n",
    "    MEDIAN_AGE NUMERIC(7,4),\n",
    "    MALE_POP NUMERIC(20,4),\n",
    "    FEMALE_POP NUMERIC(20,4),\n",
    "    TOTAL_POP NUMERIC(20,4),\n",
    "    NUM_VETERANS NUMERIC(20,4),\n",
    "    FOREIGN_BORN NUMERIC(20,4),\n",
    "    AVG_HOUSEHOLD_SIZE NUMERIC(7,3),\n",
    "    RACE VARCHAR,\n",
    "    COUNT NUMERIC(20,4),\n",
    "    PRIMARY KEY(CITY, STATE_CODE)\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(demographic_dim_drop)\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(demographic_dim_create)\n",
    "conn.commit()\n",
    "\n",
    "demo_dim_insert = (\"\"\"INSERT INTO CITY_STATE_DEMOGRAPHIC_DIM(CITY, STATE_CODE,STATE,MEDIAN_AGE, MALE_POP,FEMALE_POP, TOTAL_POP,\n",
    "                    NUM_VETERANS, FOREIGN_BORN, AVG_HOUSEHOLD_SIZE, RACE, COUNT\n",
    "                    ) VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) \n",
    "                    ON CONFLICT(CITY, STATE_CODE) DO UPDATE\n",
    "                        SET MEDIAN_AGE = EXCLUDED.MEDIAN_AGE,\n",
    "                            MALE_POP = EXCLUDED.MALE_POP,\n",
    "                            FEMALE_POP = EXCLUDED.FEMALE_POP,\n",
    "                            TOTAL_POP = EXCLUDED.TOTAL_POP,\n",
    "                            NUM_VETERANS = EXCLUDED.NUM_VETERANS,\n",
    "                            FOREIGN_BORN = EXCLUDED.FOREIGN_BORN,\n",
    "                            AVG_HOUSEHOLD_SIZE = EXCLUDED.AVG_HOUSEHOLD_SIZE,\n",
    "                            RACE = EXCLUDED.RACE,\n",
    "                            COUNT = EXCLUDED.COUNT\n",
    "\"\"\")\n",
    " \n",
    "demo_df = df[['City', 'State Code', 'State', 'Median Age', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'Average Household Size', 'Race', 'Count']]\n",
    "for i, row in demo_df.iterrows():\n",
    "    cur.execute(demo_dim_insert, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITY  Carolina\n",
      "STATE_CODE  PR\n",
      "MEDIAN_AGE  42.0000\n",
      "CITY  North Little Rock\n",
      "STATE_CODE  AR\n",
      "MEDIAN_AGE  33.6000\n",
      "CITY  Las Cruces\n",
      "STATE_CODE  NM\n",
      "MEDIAN_AGE  32.7000\n",
      "CITY  San Bernardino\n",
      "STATE_CODE  CA\n",
      "MEDIAN_AGE  28.5000\n",
      "CITY  Champaign\n",
      "STATE_CODE  IL\n",
      "MEDIAN_AGE  28.7000\n"
     ]
    }
   ],
   "source": [
    "#cur.execute(\"select * from CITY_STATE_DIM limit 5\")\n",
    "#records = cur.fetchall()\n",
    "#for record in records:\n",
    "#    print(\"CITY \", record[0])\n",
    "#    print(\"STATE \", record[1])\n",
    "#    print(\"STATE_CODE \", record[2])\n",
    "    \n",
    "#check if duplicate states got inserted, West Virginia is not in the list of states\n",
    "#cur.execute(\"select distinct state from CITY_STATE_DIM order by state\")\n",
    "#records = cur.fetchall()\n",
    "#for record in records:\n",
    "#    print(\"State\", record[0])\n",
    "\n",
    "cur.execute(\"select * from CITY_STATE_DEMOGRAPHIC_DIM limit 5\")\n",
    "records = cur.fetchall()\n",
    "for record in records:\n",
    "    #print(record)\n",
    "    print(\"CITY \", record[0])\n",
    "    print(\"STATE_CODE \", record[1])\n",
    "    print(\"MEDIAN_AGE \", record[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Visa_Type_Dim\n",
    "fname = 'I94_Visa.csv'\n",
    "df = pd.read_csv(fname)\n",
    "df.head()\n",
    "\n",
    "VISA_TYPE_DIM_drop = \"DROP TABLE IF EXISTS VISA_TYPE_DIM\"\n",
    "\n",
    "VISA_TYPE_DIM_Create = (\"\"\"CREATE TABLE IF NOT EXISTS VISA_TYPE_DIM(\n",
    "    VISA_CODE INTEGER,\n",
    "    VISA_TYPE VARCHAR,\n",
    "    PRIMARY KEY(VISA_TYPE)\n",
    ")\"\"\")\n",
    "cur.execute(VISA_TYPE_DIM_drop)\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(VISA_TYPE_DIM_Create)\n",
    "conn.commit()\n",
    "\n",
    "visa_type_dim_insert = (\"\"\"INSERT INTO VISA_TYPE_DIM(VISA_CODE, VISA_TYPE) VALUES(%s, %s) ON CONFLICT(VISA_TYPE) \n",
    "                            DO NOTHING\n",
    "\"\"\")\n",
    "\n",
    "visa_df = df[['value', 'I94Visa']]\n",
    "for i, row in visa_df.iterrows():\n",
    "    cur.execute(visa_type_dim_insert, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Business')\n",
      "(2, 'Pleasure')\n",
      "(3, 'Student')\n"
     ]
    }
   ],
   "source": [
    "cur.execute(\"select * from VISA_TYPE_DIM\")\n",
    "records = cur.fetchall()\n",
    "for record in records:\n",
    "    print(record)\n",
    "    #print(\"CODE \", record[0])\n",
    "    #print(\"NAME \", record[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code, name\n",
      "('ALC', 'ALCAN,AK')\n",
      "('ANC', 'ANCHORAGE,AK')\n",
      "('BAR', 'BAKERAAF-BAKERISLAND,AK')\n",
      "('DAC', 'DALTONSCACHE,AK')\n",
      "('PIZ', 'DEWSTATIONPTLAYDEW,AK')\n",
      "('DTH', 'DUTCHHARBOR,AK')\n",
      "('EGL', 'EAGLE,AK')\n",
      "('FRB', 'FAIRBANKS,AK')\n",
      "('HOM', 'HOMER,AK')\n",
      "('HYD', 'HYDER,AK')\n"
     ]
    }
   ],
   "source": [
    "#Port_Dim\n",
    "fname = 'I94_Port.csv'\n",
    "df = pd.read_csv(fname, delimiter=';')\n",
    "df.head()\n",
    "\n",
    "PORT_DIM_drop = \"DROP TABLE IF EXISTS PORT_DIM\"\n",
    "\n",
    "PORT_DIM_Create = (\"\"\"CREATE TABLE IF NOT EXISTS PORT_DIM(\n",
    "    PORT_CODE VARCHAR,\n",
    "    PORT_NAME VARCHAR,\n",
    "    PRIMARY KEY(PORT_CODE)\n",
    ")\"\"\")\n",
    "cur.execute(PORT_DIM_drop)\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(PORT_DIM_Create)\n",
    "conn.commit()\n",
    "\n",
    "port_dim_insert = (\"\"\"INSERT INTO PORT_DIM(PORT_CODE, PORT_NAME) VALUES(%s, %s) ON CONFLICT(PORT_CODE) \n",
    "                            DO NOTHING\n",
    "\"\"\")\n",
    "df['value'] = df['value'].str.replace(r\"[\\',]\", '')\n",
    "#print(df)\n",
    "df['i94prtl'] = df['i94prtl'].str.replace(r\"[\\']\",'')\n",
    "#df['i94prtl'] = df['i94prtl'].str.replace(r\"[,]\",' ')\n",
    "\n",
    "df.head()\n",
    "for i, row in df.iterrows():\n",
    "    cur.execute(port_dim_insert, row)\n",
    "cur.execute(\"select * from PORT_DIM limit 10\")\n",
    "records = cur.fetchall()\n",
    "print(\"code, name\")\n",
    "for record in records:\n",
    "    print(record)\n",
    "    #print(\"CODE \", record[0])\n",
    "    #print(\"NAME \", record[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#City_Res_Dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_code, state\n",
      "(\"'AL'\", \"'ALABAMA'\")\n",
      "(\"'AK'\", \"'ALASKA'\")\n",
      "(\"'AZ'\", \"'ARIZONA'\")\n",
      "(\"'AR'\", \"'ARKANSAS'\")\n",
      "(\"'CA'\", \"'CALIFORNIA'\")\n",
      "(\"'CO'\", \"'COLORADO'\")\n",
      "(\"'CT'\", \"'CONNECTICUT'\")\n",
      "(\"'DE'\", \"'DELAWARE'\")\n",
      "(\"'DC'\", \"'DIST. OF COLUMBIA'\")\n",
      "(\"'FL'\", \"'FLORIDA'\")\n"
     ]
    }
   ],
   "source": [
    "#addr_state_dim\n",
    "fname = 'i94ADDR_State.csv'\n",
    "df = pd.read_csv(fname)\n",
    "df.head()\n",
    "\n",
    "addr_state_dim_drop = \"DROP TABLE IF EXISTS addr_state_dim\"\n",
    "\n",
    "addr_state_dim_Create = (\"\"\"CREATE TABLE IF NOT EXISTS addr_state_dim(\n",
    "\tSTATE_CODE VARCHAR,\n",
    "\tSTATE VARCHAR,\n",
    "\tPRIMARY KEY(STATE_CODE)\n",
    ")\"\"\")\n",
    "cur.execute(addr_state_dim_drop)\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(addr_state_dim_Create)\n",
    "conn.commit()\n",
    "\n",
    "addr_state_dim_insert = (\"\"\"INSERT INTO addr_state_dim(state_code, state) VALUES(%s, %s) ON CONFLICT(state_code) \n",
    "                            DO NOTHING\n",
    "\"\"\")\n",
    "for i, row in df.iterrows():\n",
    "    cur.execute(addr_state_dim_insert, row)\n",
    "cur.execute(\"select * from addr_state_dim limit 10\")\n",
    "records = cur.fetchall()\n",
    "print(\"state_code, state\")\n",
    "for record in records:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['value', 'i94cntyl']\n",
      "res_code, res_state\n",
      "('582', 'MEXICO Air Sea, and Not Reported (I-94, no land arrivals)')\n",
      "('236', 'AFGHANISTAN')\n",
      "('101', 'ALBANIA')\n",
      "('316', 'ALGERIA')\n",
      "('102', 'ANDORRA')\n",
      "('324', 'ANGOLA')\n",
      "('529', 'ANGUILLA')\n",
      "('518', 'ANTIGUA-BARBUDA')\n",
      "('687', 'ARGENTINA ')\n",
      "('151', 'ARMENIA')\n"
     ]
    }
   ],
   "source": [
    "#Res_City_Country_dim\n",
    "#I94City_Res.csv\n",
    "fname = 'I94City_Res.csv'\n",
    "df = pd.read_csv(fname, delimiter=\";\")\n",
    "\n",
    "print(df.columns.tolist())\n",
    "\n",
    "df['i94cntyl'] = df['i94cntyl'].str.replace(r\"[\\']\", \"\")\n",
    "df.head()\n",
    "\n",
    "Res_City_Country_drop = \"DROP TABLE IF EXISTS Res_City_Country_dim\"\n",
    "#\n",
    "Res_City_Country_Create = (\"\"\"CREATE TABLE IF NOT EXISTS Res_City_Country_dim(\n",
    "\tRES_CODE VARCHAR,\n",
    "\tRES_STATE VARCHAR,\n",
    "\tPRIMARY KEY(RES_CODE)\n",
    ")\"\"\")\n",
    "cur.execute(Res_City_Country_drop)\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(Res_City_Country_Create)\n",
    "conn.commit()\n",
    "\n",
    "Res_City_Country_dim_insert = (\"\"\"INSERT INTO Res_City_Country_dim(RES_CODE, RES_STATE) VALUES(%s, %s) ON CONFLICT(RES_CODE) \n",
    "                            DO NOTHING\n",
    "\"\"\")\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    cur.execute(Res_City_Country_dim_insert, row)\n",
    "\n",
    "cur.execute(\"select * from Res_City_Country_dim limit 10\")\n",
    "records = cur.fetchall()\n",
    "\n",
    "print(\"res_code, res_state\")\n",
    "for record in records:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fact table to load the SAS i94 file\n",
    "df_spark=spark.read.parquet(\"sas_data_parquet\")\n",
    "df_count = spark.sql(\"\"\"select * from immi_fact limit 1\"\"\")\n",
    "df_count.show()\n",
    "\n",
    "query =  \"DROP TABLE IF EXISTS Immigrant_stage\"\n",
    "\n",
    "cur.execute(query)\n",
    "conn.commit()\n",
    "\n",
    "Immigrant_stage_Create = (\"\"\"CREATE TABLE IF NOT EXISTS Immigrant_stage(\n",
    "    cicid BIGINT, i94yr INTEGER, i94mon INTEGER, i94cit INTEGER,\n",
    "    i94res INTEGER,  i94port VARCHAR, arrdate INTEGER, i94mode INTEGER,\n",
    "    i94addr VARCHAR, depdate INTEGER, i94bir INTEGER,  i94visa INTEGER,\n",
    "    count INTEGER, dtadfile INTEGER, visapost VARCHAR, occup VARCHAR,\n",
    "    entdepa VARCHAR, entdepd VARCHAR, entdepu VARCHAR, matflag VARCHAR,\n",
    "    biryear INTEGER, dtaddto INTEGER, gender VARCHAR, insnum BIGINT,\n",
    "    airline VARCHAR,  admnum BIGINT, fltno VARCHAR, visatype VARCHAR)\"\"\")\n",
    "\n",
    "cur.execute(query)\n",
    "conn.commit()\n",
    "\n",
    "#df = df_count[['cicid', 'i94yr', 'i94mon','i94cit', 'i94res', 'i94port', 'arrdate','i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count', 'dtadfile', 'visapost', 'entdepa',\n",
    "#'entdepd', 'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'airline', 'admnum', 'fltno', 'visatype']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Immigrant_Fact_drop = \"DROP TABLE IF EXISTS Immigrant_Fact\"\n",
    "#\n",
    "Immigrant_Fact_Create = (\"\"\"CREATE TABLE IF NOT EXISTS Immigrant_Fact(\n",
    "    CICID BIGINT, I94YR INTEGER, I94MON INTEGER, I94CIT INTEGER, I94RES INTEGER,\n",
    "    I94PORT VARCHAR, ARRDATE INTEGER, I94MODE INTEGER, I94ADDR VARCHAR,\n",
    "    DEPDATE INTEGER, I94BIR INTEGER, I94VISA INTEGER, COUNT INTEGER,\n",
    "    DTADFILE BIGINT, VISAPOST VARCHAR, ENTDEPA VARCHAR, ENTDEPD VARCHAR,\n",
    "    ENTDEPU VARCHAR, MATFLAG VARCHAR, BIRYEAR INTEGER, DTADDTO BIGINT,\n",
    "    GENDER VARCHAR, AIRLINE VARCHAR, ADMNUM DECIMAL, FLTNO INTEGER,\n",
    "    VISATYPE VARCHAR, PRIMARY KEY(CICID)\n",
    ")\"\"\")\n",
    "cur.execute(Immigrant_Fact_drop)\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(Immigrant_Fact_Create)\n",
    "conn.commit()\n",
    "\n",
    "Immigrant_Fact_insert = (\"\"\"INSERT INTO Immigrant_Fact(CICID, I94YR, I94MON,I94CIT,I94RES, I94PORT, ARRDATE,\n",
    "                            I94MODE, I94ADDR, DEPDATE, I94BIR, I94VISA, COUNT, DTADFILE, VISAPOST, ENTDEPA,\n",
    "                            ENTDEPD, ENTDEPU, MATFLAG, BIRYEAR, DTADDTO, GENDER, AIRLINE, ADMNUM, FLTNO, VISATYPE \n",
    "                        ) VALUES(%s, %s,%s, %s,%s, %s,%s, %s,%s, %s,%s, %s,%s, %s,%s, %s,%s, %s,%s, %s, %s, %s,%s, %s,%s, %s) ON CONFLICT(CICID) \n",
    "                            DO NOTHING\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cur.execute(\"select * from Immigrant_Fact limit 10\")\n",
    "records = cur.fetchall()\n",
    "\n",
    "for record in records:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    * I chose Redshift and S3 for this project as it is good to be able to access source data and have a robust RDBMS for loading dimension and fact table so that analysis can be done\n",
    "* Propose how often the data should be updated and why.\n",
    "    * If this is for analytical purposes, fact data should be updated at least on a monthly basis\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    "     * The data was increased by 100x.\n",
    "     * Make sure that S3 and Redshift capacity is scaled to match the volume of the data\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.  \n",
    "     * Make sure the source data arrives to the S3 bucket at least 2 hours before the dashboard should be made available\n",
    "     * Schedule the scripts (in Airflow or native scheduler) to load data to Redshift so that the data loads and quality checks are complete by 7 am\n",
    " * The database needed to be accessed by 100+ people.\n",
    "     * The data is distributed and database can scale to accommodate the number of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
